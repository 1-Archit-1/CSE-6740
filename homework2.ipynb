{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94e855a5",
   "metadata": {},
   "source": [
    "## HW2 of CSE 6740\n",
    "## Programming: Text Clustering\n",
    "In this problem, we will explore the use of EM algorithm for text clustering. Text clustering is a technique for unsupervised document organization, information retrieval. We want to find how to group a set of different text documents based on their topics. First we will analyze a model to represent the data.\n",
    "\n",
    "### Bag of Words\n",
    "The simplest model for text documents is to understand them as a collection of words. To keep the model simple, we keep the collection unordered, disregarding grammar and word order. What we do is counting how often each word appears in each document and store the word counts into a matrix, where each row of the matrix represents one document. Each column of matrix represent a specific word from the document dictionary. Suppose we represent the set of nd documents using a matrix of word counts like this:\n",
    "\n",
    "$$\n",
    "D_{1: n_d}=\\left(\\begin{array}{cccc}\n",
    "2 & 6 & \\ldots & 4 \\\\\n",
    "2 & 4 & \\ldots & 0 \\\\\n",
    "\\vdots & & \\ddots &\n",
    "\\end{array}\\right)=T\n",
    "$$\n",
    "\n",
    "\n",
    "This means that word $W_1$ occurs twice in document $D_1$. Word $W_{n_w}$ occurs 4 times in document $D_1$ and not at all in document $D_2$.\n",
    "\n",
    "### Multinomial Distribution\n",
    "The simplest distribution representing a text document is multinomial distribution (Bishop Chapter 2.2). The probability of a document $D_i$ is:\n",
    "\n",
    "$$\n",
    "p\\left(D_i\\right)=\\prod_{j=1}^{n_w} \\mu_j^{T_{i j}}\n",
    "$$\n",
    "\n",
    "\n",
    "Here, $\\mu_j$ denotes the probability of a particular word in the text being equal to $w_j, T_{i j}$ is the count of the word in document. So the probability of document $D_1$ would be $p\\left(D_1\\right)=\\mu_1^2 \\cdot \\mu_2^6 \\cdot \\ldots \\cdot \\mu_{n_w}^4$.\n",
    "\n",
    "### Mixture of Multinomial Distributions\n",
    "In order to do text clustering, we want to use a mixture of multinomial distributions, so that each topic has a particular multinomial distribution associated with it, and each document is a mixture of different topics. We define $p(c)=\\pi_c$ as the mixture coefficient of a document containing topic $c$, and each topic is modeled by a multinomial distribution $p\\left(D_i \\mid c\\right)$ with parameters $\\mu_{j c}$, then we can write each document as a mixture over topics as\n",
    "\n",
    "$$\n",
    "p\\left(D_i\\right)=\\sum_{c=1}^{n_c} p\\left(D_i \\mid c\\right) p(c)=\\sum_{c=1}^{n_c} \\pi_c \\prod_{j=1}^{n_w} \\mu_{j c}^{T_{i j}}\n",
    "$$\n",
    "\n",
    "\n",
    "### EM for Mixture of Multinomials\n",
    "In order to cluster a set of documents, we need to fit this mixture model to data. In this problem, the EM algorithm can be used for fitting mixture models. This will be a simple topic model for documents. Each topic is a multinomial distribution over words (a mixture component). EM algorithm for such a topic model, which consists of iterating the following steps:\n",
    "\n",
    "**Expectation**\n",
    "\n",
    "Compute the expectation of document $D_i$ belonging to cluster $c$ :\n",
    "\n",
    "$$\n",
    "\\gamma_{i c}=\\frac{\\pi_c \\prod_{j=1}^{n_w} \\mu_{j c}^{T_{i j}}}{\\sum_{c=1}^{n_c} \\pi_c \\prod_{j=1}^{n_w} \\mu_{j c}^{T_{i j}}}\n",
    "$$\n",
    "\n",
    "**Maximization**\n",
    "\n",
    "Update the mixture parameters, i.e. the probability of a word being $W_j$ in cluster (topic) $c$, as well as prior probability of each cluster.\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\mu_{j c}=\\frac{\\sum_{i=1}^{n_d} \\gamma_{i c} T_{i j}}{\\sum_{i=1}^{n_d} \\sum_{l=1}^{m_w} \\gamma_{i c} T_{i l}} \\\\\n",
    "\\pi_c=\\frac{1}{n_d} \\sum_{i=1}^{n_d} \\gamma_{i c}\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "\n",
    "### Task\n",
    "\n",
    "Implement the algorithm and run on the toy dataset `data.mat`. Observe the results and compare them with the provided true clusters each document belongs to. Report the evaluation (e.g. accuracy) of your implementation.\n",
    "Hint: We already did the word counting for you, so the data file only contains a count matrix like the one shown above. For the toy dataset, set the number of clusters $n_c$ = 4. You will need to initialize the parameters. Try several different random initial values for the probability of a word being $W_j$ in topic c, $μ_{jc}$. Make sure you normalized it. Make sure that you should not use the true cluster information during your learning phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "90eb4740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged\n",
      "0 : accuracy 0.8750\n",
      "Converged\n",
      "1 : accuracy 0.8150\n",
      "Converged\n",
      "2 : accuracy 0.8025\n",
      "Converged\n",
      "3 : accuracy 0.8200\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "def acc_measure(idx, Y):\n",
    "    \"\"\"\n",
    "    Function for evaluate the accuracy\n",
    "    :param idx:\n",
    "        numpy array of (num_doc)\n",
    "    :param Y:\n",
    "        numpy array of (num_doc)\n",
    "    :return:\n",
    "        accuracy\n",
    "    \"\"\"\n",
    "    # rotate for different idx assignments\n",
    "    best_acc = 0\n",
    "    for idx_order in itertools.permutations([1, 2, 3, 4]):\n",
    "\n",
    "        for ind, label in enumerate(idx_order):\n",
    "            Y[(ind)*100:(ind+1)*100] = label\n",
    "\n",
    "        acc = (Y == idx).sum() / Y.shape[0]\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "\n",
    "    return best_acc\n",
    "\n",
    "# Example:\n",
    "#     acc_measure(np.array([1]*100 + [3]*100 + [2]*100 + [4]*100))\n",
    "\n",
    "def cluster(bow, K, num_iters = 1000, epsilon = 1e-12):\n",
    "    \"\"\"\n",
    "    # TODO: Finish the cluster function\n",
    "    :param bow:\n",
    "        bag-of-word matrix of (num_doc, V), where V is the vocabulary size\n",
    "    :param K:\n",
    "        number of topics\n",
    "    :return:\n",
    "        idx of size (num_doc), idx should be 1, 2, 3 or 4\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    num_docs, num_words = bow.shape\n",
    "        \n",
    "    # Initialize parameters\n",
    "    pi = np.random.rand(K)\n",
    "    pi /= np.sum(pi) \n",
    "\n",
    "    mu = np.random.rand(K, num_words)\n",
    "    mu /= mu.sum(axis=1, keepdims=True)  \n",
    "\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        # E-Step\n",
    "        gamma = pi * np.prod(mu ** bow[:, np.newaxis, :], axis=2)  # Shape: (num_docs, K)\n",
    "        gamma /= gamma.sum(axis=1, keepdims=True)  # Normalize \n",
    "        old_pi = pi.copy()\n",
    "        old_mu = mu.copy()\n",
    "        # M-Step: Update pi and mu \n",
    "        pi = gamma.sum(axis=0) / num_docs  # Shape: (K,)\n",
    "\n",
    "        mu = (gamma.T @ bow)  # Shape: (K, num_words)\n",
    "        mu /= mu.sum(axis=1, keepdims=True)  # Normalize across words for each topic\n",
    "        #check for convergence\n",
    "        if np.linalg.norm(pi - old_pi) < epsilon and np.linalg.norm(mu - old_mu) < epsilon:\n",
    "            print('Converged')\n",
    "            break\n",
    "        \n",
    "\n",
    "\n",
    "    # Assign each document\n",
    "    idx = np.argmax(gamma, axis=1) + 1  # Add 1 to make the labels 1-based\n",
    "    return idx\n",
    "\n",
    "# Evaluation\n",
    "mat = scipy.io.loadmat('data.mat')\n",
    "mat = mat['X']\n",
    "X = mat[:, :-1]\n",
    "Y = mat[:, -1]\n",
    "for i in range (4):\n",
    "    idx = cluster(X, 4)\n",
    "    acc = acc_measure(idx, Y)\n",
    "    print(f'{i} : accuracy %.4f' % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aece49e5",
   "metadata": {},
   "source": [
    "## Extra Credit: Realistic Topic Models\n",
    "The above model assumes all the words in a document belongs to some topic at the same time. However, in real world datasets, it is more likely that some words in the documents belong to one topic while other words belong to some other topics. For example, in a news report, some words may talk about “Ebola” and “health”, while others may mention “administration” and “congress”. In order to model this phenomenon, we should model each word as a mixture of possible topics. \n",
    "\n",
    "Specifically, consider the log-likelihood of the joint distribution of document and words\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{L}=\\sum_{d \\in \\mathcal{D}} \\sum_{w \\in \\mathcal{W}} T_{d w} \\log P(d, w)\n",
    "$$\n",
    "\n",
    "where $T_{d w}$ is the counts of word $w$ in the document $d$. This count matrix is provided as input.\n",
    "The joint distribution of a specific document and a specific word is modeled as a mixture\n",
    "\n",
    "$$\n",
    "P(d, w)=\\sum_{z \\in \\mathcal{Z}} P(z) P(w \\mid z) P(d \\mid z)\n",
    "$$\n",
    "\n",
    "where $P(z)$ is the mixture proportion, $P(w \\mid z)$ is the distribution over the vocabulary for the $z$-th topic, and $P(d \\mid z)$ is the probability of the document for the $z$-th topic. And these are the parameters for the model.\n",
    "\n",
    "The E-step calculates the posterior distribution of the latent variable conditioned on all other variables\n",
    "\n",
    "$$\n",
    "P(z \\mid d, w)=\\frac{P(z) P(w \\mid z) P(d \\mid z)}{\\sum_{z^{\\prime}} P\\left(z^{\\prime}\\right) P\\left(w \\mid z^{\\prime}\\right) P\\left(d \\mid z^{\\prime}\\right)}\n",
    "$$\n",
    "\n",
    "\n",
    "In the M-step, we maximizes the expected complete log-likelihood with respect to the parameters, and get the following update rules\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(w \\mid z) & =\\frac{\\sum_d T_{d w} P(z \\mid d, w)}{\\sum_{w^{\\prime}} \\sum_d T_{d w^{\\prime}} P\\left(z \\mid d, w^{\\prime}\\right)} \\\\\n",
    "P(d \\mid z) & =\\frac{\\sum_w T_{d w} P(z \\mid d, w)}{\\sum_{d^{\\prime}} \\sum_w T_{d^{\\prime} w} P\\left(z \\mid d^{\\prime}, w\\right)} \\\\\n",
    "P(z) & =\\frac{\\sum_d \\sum_w T_{d w} P(z \\mid d, w)}{\\sum_{z^{\\prime}} \\sum_{d^{\\prime}} \\sum_{w^{\\prime}} T_{d^{\\prime} w^{\\prime}} P\\left(z^{\\prime} \\mid d^{\\prime}, w^{\\prime}\\right)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## Task\n",
    "Implement EM for maximum likelihood estimation and cluster the text data provided in the nips.mat file you downloaded. You can print out the top key words for the topics/clusters by using the show `display_topics` utility. It takes two parameters: 1) your learned conditional distribution matrix, i.e., $P(w|z)$ and 2) a cell array of words that corresponds to the vocabulary. You can find the cell array wl in the nips.mat file. Try different values of $k$ and see which values produce sensible topics. In assessing your code, we will use another dataset and observe the produces topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9817bfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Number of topics: 2\n",
      "topic 0: learning model input neural networks data network figure set time\n",
      "topic 1: network output training neural point parameters error examples function learning\n",
      "\n",
      " Number of topics: 3\n",
      "topic 0: model neural time training set figure data learning output function\n",
      "topic 1: learning set network input algorithm neural time system number data\n",
      "topic 2: network model learning figure input function output networks results data\n",
      "\n",
      " Number of topics: 4\n",
      "topic 0: learning neural training algorithm data function model based system order\n",
      "topic 1: model input network set time figure output networks training state\n",
      "topic 2: network information time neural set learning input networks hidden model\n",
      "topic 3: learning network neural function figure data error model system networks\n",
      "\n",
      " Number of topics: 5\n",
      "topic 0: network input model learning data output figure training state set\n",
      "topic 1: networks neural model function network time number figure input learning\n",
      "topic 2: network learning neural function data algorithm set training system information\n",
      "topic 3: time model learning set figure neural data hidden training input\n",
      "topic 4: network networks error time figure learning training algorithm weights number\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "def cluster_extra(T, K, num_iters = 50, epsilon = 1e-12):\n",
    "    \"\"\"\n",
    "    TODO: Finish the function of clustering.\n",
    "    :param bow:\n",
    "        bag-of-word matrix of (num_doc, V), where V is the vocabulary size\n",
    "    :param K:\n",
    "        number of topics\n",
    "    :return:\n",
    "        word-topic matrix of (V, K)\n",
    "    \"\"\"\n",
    "    T=T.toarray()\n",
    "    num_docs, vocab_size = T.shape\n",
    "    Word_topic_matrix = np.random.rand(vocab_size, K)  # P(w|z)\n",
    "    Doc_topic_probs = np.random.rand(num_docs, K)   # P(d|z)\n",
    "    Topic_probabilities = np.random.rand(K)  # P(z)\n",
    "\n",
    "    # Normalize the initial distributions\n",
    "    Word_topic_matrix /= Word_topic_matrix.sum(axis=0)  \n",
    "    Doc_topic_probs /= Doc_topic_probs.sum(axis=0)  \n",
    "    Topic_probabilities /= Topic_probabilities.sum()        \n",
    "\n",
    "    # EM loop\n",
    "    for iteration in range(num_iters):\n",
    "        \n",
    "        # E-Step: Compute responsibilities P(z | d, w)\n",
    "\n",
    "        Responsibility_matrix = Topic_probabilities * Doc_topic_probs[:, np.newaxis, :] * Word_topic_matrix[np.newaxis, :, :]\n",
    "        Responsibility_matrix /= Responsibility_matrix.sum(axis=2, keepdims=True)  # Normalize\n",
    "        \n",
    "        # M-Step\n",
    "        matrix = (T[:, :, np.newaxis] * Responsibility_matrix)\n",
    "        # Update P(w|z)\n",
    "        New_word_topic_matrix= matrix.sum(axis=0)\n",
    "        New_word_topic_matrix /= New_word_topic_matrix.sum(axis=0)\n",
    "        # Update P(d|z)\n",
    "        New_doc_topic_probs = matrix.sum(axis=1)\n",
    "        New_doc_topic_probs /= New_doc_topic_probs.sum(axis=0)\n",
    "        # Update P(z)\n",
    "        New_topic_prob = matrix.sum(axis=(0, 1))\n",
    "        New_topic_prob /= New_topic_prob.sum()\n",
    "\n",
    "        # Check for convergence\n",
    "        x= np.linalg.norm(New_topic_prob-Topic_probabilities)\n",
    "        if x < 1e-4:\n",
    "            print(\"Converged\")\n",
    "            break\n",
    "\n",
    "        # Update parameters\n",
    "        Word_topic_matrix, Doc_topic_probs, Topic_probabilities = New_word_topic_matrix, New_doc_topic_probs, New_topic_prob\n",
    "\n",
    "    return Word_topic_matrix\n",
    "    \n",
    "   \n",
    "\n",
    "def display_topics(W, wl):\n",
    "    \"\"\"\n",
    "\n",
    "    :param W:\n",
    "        word-topic matrix of size (V, K)\n",
    "    :param wl:\n",
    "        array of str of size (V)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    top_n_words = 10\n",
    "    ind_mat = np.argsort(-W.T, axis=1)[:, :top_n_words]\n",
    "    for k_ind in range(ind_mat.shape[0]):\n",
    "        w_ls = [wl[ind, 0][0] for ind in ind_mat[k_ind]]\n",
    "        print('topic %i: %s' % (k_ind, ' '.join(w_ls)))\n",
    "        \n",
    "\n",
    "## Evaluation for displaying topics\n",
    "n_topics = 5 # TODO specify num topics yourself\n",
    "cell = scipy.io.loadmat('nips.mat')\n",
    "\n",
    "mat = cell['raw_count'] # sparse mat of size (num_doc, num_words)\n",
    "wl = cell['wl']\n",
    "\n",
    "for n_topics in range(2,6):\n",
    "    print(f'\\n Number of topics: {n_topics}')\n",
    "    W = cluster_extra(mat, n_topics)\n",
    "    display_topics(W, wl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
